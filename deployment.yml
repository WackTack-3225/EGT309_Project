# Min Zheng - Flask app

# Define the Namespace
apiVersion: v1
kind: Namespace # abstraction layer to prevent pods from being build in base env to prevent accidental errors
metadata:
  name: ml-app

---

apiVersion: v1
kind: ConfigMap # useful tool to allow easier access & modification of URL/File paths 
metadata:
  name: flask-url
  namespace: ml-app
data:
  DP_POD_URL: "http://processing-service:84/start-processing" # WORKING
  MODEL_TRAINING_URL: "http://model-training-service:85/train" # SET TO CORRECT (?)
  RESULTS_URL: "http://model-training-service:85/results" # SET TO CORRECT(?)
  INF_POD_URL: "http://inference-service:83/predict" # WORKING

---
# Flask App Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: flask-app
  namespace: ml-app
spec:
  replicas: 3 # MAX 3 containers deployed
  selector:
    matchLabels:
      chapter: flask # Applies configuration to containers w/ label flask
  strategy:
    type: RollingUpdate # Deploy/Remove/Update pods based on specs below
    rollingUpdate:
      maxUnavailable: 1 # max 1 pod unavail at a time
      maxSurge: 1 # max 1 extra pod avail at a time
  template:
    metadata:
      labels:
        chapter: flask # create from a temaplate (image) and apply label flask for svc to connect
    spec:
      containers:
      - name: flask-container
        image: edysontan/flask-app:latest # pull from flask-app image
        env: # to get Config map variables
        - name: DP_POD_URL
          valueFrom:
            configMapKeyRef:
              name: flask-url
              key: DP_POD_URL
        - name: MODEL_TRAINING_URL
          valueFrom:
            configMapKeyRef:
              name: flask-url
              key: MODEL_TRAINING_URL
        - name: RESULTS_URL
          valueFrom:
            configMapKeyRef:
              name: flask-url
              key: RESULTS_URL
        - name: INF_POD_URL
          valueFrom:
            configMapKeyRef:
              name: flask-url
              key: INF_POD_URL
        ports:
        - containerPort: 5000 # set port 5000, same as app.py

---
# Flask app service
apiVersion: v1
kind: Service
metadata:
  name: flask-app-service
  namespace: ml-app # deploy in same namespace to access
  labels:
    chapter: flask # unique calling method to ensure service operates on this specific chapter
spec:
  type: NodePort  # LoadBalancer for external
  ports:
  - protocol: TCP
    port: 5000  # Exposed port
    targetPort: 5000  # Flask app port inside the container
  selector:
    chapter: flask # applies to selected chapter: flask

---
# Inference Container - Edyson
# Configmap

# Store non-confidential data

apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
  namespace: ml-app        # The namespace in which the Service is created
data:
  DATA_PATH: "/mnt/data"
  MODEL_PATH: "/mnt/save-model"
  VOLUME_MOUNT_PATH: "/mnt"


---

# Service

# Service accessible from within the cluster (ClusterIP) and will sent traffic to pods
# it manages (recieves on 80 sent to containers at 8083)
# Does not deploy or manage pods just route traffic
# If can control >1 pod it will auto route traffic to them

apiVersion: v1  # Specifies the API version to use for the Service object
kind: Service   # Defines this as a Service object
metadata:
  name: inference-service  # The name of the Service
  namespace: ml-app        # The namespace in which the Service is created
spec:
  selector:
    app: inference         # Labels used to identify the pods this Service targets
  ports:
  - protocol: TCP          # The protocol for the port (usually TCP)
    port: 83               # The port that the Service exposes to the outside world
    targetPort: 8083       # The port on the pod/container that the Service forwards traffic to
  type: ClusterIP          # The type of Service; ClusterIP means it's accessible only within the cluster

---

# PVC

# To allow request of a storage, request 4 Gi

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: store-model-pvc
  namespace: ml-app        # The namespace in which the PVC is created
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 4Gi

---

# Inference Deployment

# Deployment use template to how pod shld be created
# Replicaset controller created from deployment and it ensure pod is at desired state
# and will mintor and if not it delete, create and manage pods
# Note: rollback history is gone (no history of replicasets) if deployment.yaml is deleted and reapplied, only has history if its reapplied without deleting and only has history if changes is in template

apiVersion: apps/v1 
kind: Deployment   
metadata:
  name: inference    # Name of Deployment
  namespace: ml-app  # The namespace in which the Deployment is created
spec:
  replicas: 3        # Number of pod replicas that should be running at all times
  revisionHistoryLimit: 8  # Retain the last 8 ReplicaSets for rollback (decreased from default) ensuring enough versions are kept for rollback without bloating
  minReadySeconds: 15 # Giving 15 seconds buffer time for pod to stabilize after being ready before being able to recieve traffic (Means pod marked as ready need state read for 15 seconds before being considered available to recieve traffic)
  # It in turns tells kubernetes that replica needs to run for 10 seconds before update/replace of next one in sequence
  progressDeadlineSeconds: 600  # 10 minutes to complete the update or considered as failed
  selector:
    matchLabels:
      app: inference # Label Deployment uses to manage the pods it creates
  strategy:
    type: RollingUpdate  # Deployment strategy; RollingUpdate updates the pods in a rolling fashion
    rollingUpdate:
      maxUnavailable: 1  # During an update, at most 1 pod can be unavailable at a time
      maxSurge: 1        # During an update, at most 1 extra pod can be created temporarily
      # Matching maxunavailale and maxsurge ensures stradyrollout with no reduction in total available pods
  template:
    metadata:
      name: inference-pod
      labels:
        app: inference   # Labels attached to the pods created by this Deployment
    spec:
      restartPolicy: Always #Wont restart for 1 time task
      containers:
        - name: inference-container
          image: edysontan/inference
          ports:
            - containerPort: 8083        # Port that the container listens on
          volumeMounts:
            - mountPath: /mnt  # Path inside container colume mounted at
              name: store-model-volume
          env: # environment variables for container to use
            - name: DATA_PATH
              valueFrom:
                configMapKeyRef:
                  name: app-config
                  key: DATA_PATH
            - name: MODEL_PATH
              valueFrom:
                configMapKeyRef:
                  name: app-config
                  key: MODEL_PATH
            - name: VOLUME_MOUNT_PATH
              valueFrom:
                configMapKeyRef:
                  name: app-config
                  key: VOLUME_MOUNT_PATH
      volumes: # Associate store-model-volume to store-model-pvc and volumemount mounts this associated volume to /mnt
        - name: store-model-volume
          persistentVolumeClaim:
            claimName: store-model-pvc
---


# Data Processing Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: data-processing
  namespace: ml-app
spec:
  replicas: 3
  selector:
    matchLabels:
      app: data-processing
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
      maxSurge: 1
  template:
    metadata:
      labels:
        app: data-processing
    spec:
      containers:
      - name: data-processing-container
        image: your-data-processing-image:latest
        ports:
        - containerPort: 8080

---
# Model Training Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: model-training
  namespace: ml-app
spec:
  replicas: 3
  selector:
    matchLabels:
      app: model-training
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
      maxSurge: 1
  template:
    metadata:
      labels:
        app: model-training
    spec:
      containers:
      - name: model-training-container
        image: your-model-training-image:latest
        ports:
        - containerPort: 8081

---
# Inference Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: inference
  namespace: ml-app
spec:
  replicas: 3
  selector:
    matchLabels:
      app: inference
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
      maxSurge: 1
  template:
    metadata:
      labels:
        app: inference
    spec:
      containers:
      - name: inference-container
        image: your-inference-image:latest
        ports:
        - containerPort: 8082

---
# Example Service for Data Processing
apiVersion: v1
kind: Service
metadata:
  name: data-processing-service
  namespace: ml-app
spec:
  selector:
    app: data-processing
  ports:
  - protocol: TCP
    port: 80
    targetPort: 8080
  type: ClusterIP

---
# Example Service for Model Training
apiVersion: v1
kind: Service
metadata:
  name: model-training-service
  namespace: ml-app
spec:
  selector:
    app: model-training
  ports:
  - protocol: TCP
    port: 80
    targetPort: 8081
  type: ClusterIP

---
# Example Service for Inference
apiVersion: v1
kind: Service
metadata:
  name: inference-service
  namespace: ml-app
spec:
  selector:
    app: inference
  ports:
  - protocol: TCP
    port: 80
    targetPort: 8082
  type: ClusterIP
